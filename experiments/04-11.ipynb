{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81000a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = '04-11'\n",
    "N_SPLITS = 5\n",
    "SEED = 1\n",
    "CLIP_UPPER_RATE = 1.5\n",
    "CLIP_LOWER_RATE = 1.0\n",
    "Y_THRESHOLD = 200\n",
    "EMBEDDER = 'count'\n",
    "DECOMPOSER = 'pca'  # 'pca', 'svd' or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040bdeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import configparser\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "SINCE = time.time()\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error as mse, f1_score, precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "INI_FILEPATH = os.path.join(os.path.expanduser('~'), 'aiquest2021-assesment', 'config.ini')\n",
    "config.read(INI_FILEPATH)\n",
    "if config['FOLDER']['SCRIPTS'] not in sys.path:\n",
    "    sys.path.append(config['FOLDER']['SCRIPTS'])\n",
    "from logging_util import get_logger, timer\n",
    "from feature_engineering import make_or_load_features, make_or_load_description_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ccfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(config['FOLDER']['EXPERIMENTS'], EXPERIMENT)\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd1f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, os.path.join(output_dir, 'log.log'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7074a",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f33420",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('Load dataset', logger):\n",
    "    train = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'test.csv'))\n",
    "    sample_submit = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'sample_submit.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7aa03",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93d8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(config['FOLDER']['FEATURES'], 'train_features.csv')\n",
    "test_path = os.path.join(config['FOLDER']['FEATURES'], 'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbb89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = make_or_load_features(train, test, train_path, test_path, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ea8ec43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>city</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_1st_digit</th>\n",
       "      <th>zipcode5</th>\n",
       "      <th>zipcode_imputed</th>\n",
       "      <th>has_thumbnail</th>\n",
       "      <th>first_review_year</th>\n",
       "      <th>first_review_month</th>\n",
       "      <th>host_since_year</th>\n",
       "      <th>host_since_month</th>\n",
       "      <th>last_review_year</th>\n",
       "      <th>last_review_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55578</th>\n",
       "      <td>55578</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55579</th>\n",
       "      <td>55579</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55580</th>\n",
       "      <td>55580</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55581</th>\n",
       "      <td>55581</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55582</th>\n",
       "      <td>55582</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55583 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  accommodates  bathrooms  bed_type  bedrooms  beds  \\\n",
       "0          0             6        2.0         0       1.0   4.0   \n",
       "1          1             2        1.0         0       1.0   1.0   \n",
       "2          2             2        2.0         0       1.0   1.0   \n",
       "3          3             2        1.0         0       1.0   1.0   \n",
       "4          4             2        1.0         0       1.0   1.0   \n",
       "...      ...           ...        ...       ...       ...   ...   \n",
       "55578  55578             4        1.5         0       1.0   1.0   \n",
       "55579  55579             2        1.0         0       1.0   1.0   \n",
       "55580  55580             2        1.0         0       1.0   1.0   \n",
       "55581  55581             1        1.5         0       1.0   1.0   \n",
       "55582  55582             1        1.0         0       1.0   1.0   \n",
       "\n",
       "       cancellation_policy  city  cleaning_fee  host_has_profile_pic  ...  \\\n",
       "0                        0     0             1                     1  ...   \n",
       "1                        1     1             1                     1  ...   \n",
       "2                        1     2             1                     1  ...   \n",
       "3                        1     3             1                     1  ...   \n",
       "4                        1     2             1                     1  ...   \n",
       "...                    ...   ...           ...                   ...  ...   \n",
       "55578                    1     2             1                     1  ...   \n",
       "55579                    2     4             0                     1  ...   \n",
       "55580                    0     3             1                     1  ...   \n",
       "55581                    2     0             1                     1  ...   \n",
       "55582                    0     0             0                     1  ...   \n",
       "\n",
       "       zipcode_1st_digit  zipcode5  zipcode_imputed  has_thumbnail  \\\n",
       "0                      0         0                0              1   \n",
       "1                      1         1                0              0   \n",
       "2                      2         2                1              0   \n",
       "3                      0         3                0              1   \n",
       "4                      2         4                0              0   \n",
       "...                  ...       ...              ...            ...   \n",
       "55578                  2        95                0              0   \n",
       "55579                  3       181                0              0   \n",
       "55580                  0        33                0              0   \n",
       "55581                  0       251                0              0   \n",
       "55582                  0       195                0              0   \n",
       "\n",
       "       first_review_year  first_review_month  host_since_year  \\\n",
       "0                 2016.0                 7.0           2016.0   \n",
       "1                 2016.0                 9.0           2015.0   \n",
       "2                 2016.0                 6.0           2016.0   \n",
       "3                 2014.0                 3.0           2012.0   \n",
       "4                 2015.0                 8.0           2015.0   \n",
       "...                  ...                 ...              ...   \n",
       "55578             2013.0                 2.0           2013.0   \n",
       "55579             2015.0                11.0           2015.0   \n",
       "55580             2016.0                 3.0           2016.0   \n",
       "55581             2016.0                10.0           2016.0   \n",
       "55582                NaN                 NaN           2016.0   \n",
       "\n",
       "       host_since_month  last_review_year  last_review_month  \n",
       "0                   7.0            2016.0                7.0  \n",
       "1                  12.0            2017.0                3.0  \n",
       "2                   5.0            2017.0                8.0  \n",
       "3                   6.0            2017.0                9.0  \n",
       "4                   3.0            2017.0                9.0  \n",
       "...                 ...               ...                ...  \n",
       "55578               1.0            2017.0                9.0  \n",
       "55579               1.0            2016.0               10.0  \n",
       "55580               2.0            2017.0                4.0  \n",
       "55581               4.0            2017.0                4.0  \n",
       "55582               3.0               NaN                NaN  \n",
       "\n",
       "[55583 rows x 158 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e51fe29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>city</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>...</th>\n",
       "      <th>zipcode_1st_digit</th>\n",
       "      <th>zipcode5</th>\n",
       "      <th>zipcode_imputed</th>\n",
       "      <th>has_thumbnail</th>\n",
       "      <th>first_review_year</th>\n",
       "      <th>first_review_month</th>\n",
       "      <th>host_since_year</th>\n",
       "      <th>host_since_month</th>\n",
       "      <th>last_review_year</th>\n",
       "      <th>last_review_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18523</th>\n",
       "      <td>18523</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18524</th>\n",
       "      <td>18524</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18525</th>\n",
       "      <td>18525</td>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18526</th>\n",
       "      <td>18526</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18527</th>\n",
       "      <td>18527</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18528 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  accommodates  bathrooms  bed_type  bedrooms  beds  \\\n",
       "0          0             6        2.0         0       2.0   2.0   \n",
       "1          1             3        1.0         0       1.0   1.0   \n",
       "2          2             2        1.0         0       0.0   1.0   \n",
       "3          3             4        1.0         0       1.0   2.0   \n",
       "4          4             3        1.5         0       1.0   2.0   \n",
       "...      ...           ...        ...       ...       ...   ...   \n",
       "18523  18523             4        1.0         0       2.0   2.0   \n",
       "18524  18524             2        1.0         0       2.0   1.0   \n",
       "18525  18525             5        1.5         0       2.0   2.0   \n",
       "18526  18526             2        1.0         0       1.0   2.0   \n",
       "18527  18527             1        1.0         0       1.0   1.0   \n",
       "\n",
       "       cancellation_policy  city  cleaning_fee  host_has_profile_pic  ...  \\\n",
       "0                        1     5             1                     1  ...   \n",
       "1                        2     0             1                     1  ...   \n",
       "2                        0     2             0                     1  ...   \n",
       "3                        1     2             0                     1  ...   \n",
       "4                        1     0             1                     1  ...   \n",
       "...                    ...   ...           ...                   ...  ...   \n",
       "18523                    1     2             1                     1  ...   \n",
       "18524                    0     4             0                     1  ...   \n",
       "18525                    0     4             1                     1  ...   \n",
       "18526                    1     2             1                     1  ...   \n",
       "18527                    1     0             0                     1  ...   \n",
       "\n",
       "       zipcode_1st_digit  zipcode5  zipcode_imputed  has_thumbnail  \\\n",
       "0                      4        88                0              0   \n",
       "1                      0        16                0              0   \n",
       "2                      2        90                0              0   \n",
       "3                      2        44                0              0   \n",
       "4                      0       129                0              0   \n",
       "...                  ...       ...              ...            ...   \n",
       "18523                  2       109                0              0   \n",
       "18524                  3        22                0              1   \n",
       "18525                  3       181                0              0   \n",
       "18526                  2       158                0              0   \n",
       "18527                  0       121                0              0   \n",
       "\n",
       "       first_review_year  first_review_month  host_since_year  \\\n",
       "0                 2017.0                 1.0           2016.0   \n",
       "1                 2016.0                 8.0           2014.0   \n",
       "2                    NaN                 NaN           2012.0   \n",
       "3                    NaN                 NaN           2013.0   \n",
       "4                 2015.0                 8.0           2014.0   \n",
       "...                  ...                 ...              ...   \n",
       "18523                NaN                 NaN           2009.0   \n",
       "18524             2017.0                 1.0           2017.0   \n",
       "18525                NaN                 NaN           2014.0   \n",
       "18526             2016.0                 4.0           2014.0   \n",
       "18527             2015.0                 8.0           2015.0   \n",
       "\n",
       "       host_since_month  last_review_year  last_review_month  \n",
       "0                   8.0            2017.0                9.0  \n",
       "1                   9.0            2017.0                5.0  \n",
       "2                  10.0               NaN                NaN  \n",
       "3                   1.0               NaN                NaN  \n",
       "4                  12.0            2016.0                9.0  \n",
       "...                 ...               ...                ...  \n",
       "18523              11.0               NaN                NaN  \n",
       "18524               1.0            2017.0                4.0  \n",
       "18525               9.0               NaN                NaN  \n",
       "18526               3.0            2017.0                5.0  \n",
       "18527               8.0            2017.0                1.0  \n",
       "\n",
       "[18528 rows x 158 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "313fc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns = [re.sub(r\"[:;/']\", '', c) for c in X.columns]\n",
    "X_test.columns = [re.sub(r\"[:;/']\", '', c) for c in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "026ffc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "area_features = ['latitude', 'longitude']\n",
    "\n",
    "# Onehot encoding\n",
    "categorical_features = ['cancellation_policy', 'bed_type', 'city', 'neighbourhood', 'property_type',\n",
    "                        'room_type', 'zipcode5', 'zipcode_1st_digit']\n",
    "\n",
    "# They do not need to be encoded\n",
    "int_flag_features = ['cleaning_fee', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                     'instant_bookable', 'has_thumbnail', 'zipcode_imputed']\n",
    "\n",
    "# Already one-hot style\n",
    "amenity_onehot_features = [c for c in X.columns if c.startswith('has_') and c.endswith('_amenity')]\n",
    "\n",
    "discrete_features = categorical_features + int_flag_features + amenity_onehot_features\n",
    "\n",
    "# Scaling, transformation\n",
    "continuous_features = [\n",
    "    c for c in X.columns\n",
    "    if c not in discrete_features + area_features + ['id', 'y']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6893507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 345 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reduce dimension & whitening\n",
    "## training\n",
    "decomposer = PCA(n_components=0.8, random_state=SEED, whiten=True).fit(X[amenity_onehot_features])\n",
    "X_amenity_components = decomposer.transform(X[amenity_onehot_features])\n",
    "amenity_components_columns = [f'amenity_x{i + 1}' for i in range(decomposer.n_components_)]\n",
    "X_amenity_components = pd.DataFrame(data=X_amenity_components,\n",
    "                                    columns=amenity_components_columns)\n",
    "X = pd.concat([X, X_amenity_components], axis=1)\n",
    "# X.drop(columns=amenity_onehot_features, inplace=True)\n",
    "X_test_amenity_components = decomposer.transform(X_test[amenity_onehot_features])\n",
    "X_test_amenity_components = pd.DataFrame(data=X_test_amenity_components,\n",
    "                                         columns=amenity_components_columns)\n",
    "X_test = pd.concat([X_test, X_test_amenity_components], axis=1)\n",
    "# X_test.drop(columns=amenity_onehot_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a5e95ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:[Load features] start\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\aiquest2021-assesment\\scripts\\feature_engineering.py\u001b[0m in \u001b[0;36mmake_or_load_description_vector\u001b[1;34m(train, test, feature_dir, logger, embedder, overwrite)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedder\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'doc2vec'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     return make_or_load_vector(\n\u001b[1;32m--> 199\u001b[1;33m         train, test, feature_dir, logger, 'description', embedder, vector_size, overwrite)\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\aiquest2021-assesment\\scripts\\text_feature_extraction.py\u001b[0m in \u001b[0;36mmake_or_load_vector\u001b[1;34m(train, test, feature_dir, logger, text_column, embedder, vector_size, overwrite)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# 再作成の必要が無ければ作成済みの特徴量を読み込んで終了する\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Load features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aiquest2021-assesment\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec_train, vec_test = make_or_load_description_vector(train=train,\n",
    "                                                      test=test,\n",
    "                                                      feature_dir=config['FOLDER']['FEATURES'],\n",
    "                                                      logger=logger,\n",
    "                                                      embedder=EMBEDDER)\n",
    "vec_train.shape, vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b2772e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vec_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DECOMPOSER is None:\n",
    "    assert(EMBEDDER not in ('count', 'tfidf'))\n",
    "    vec_columns = vec_train.columns.tolist()\n",
    "elif DECOMPOSER == 'pca':\n",
    "    decomposer = PCA(n_components=0.9, random_state=SEED, whiten=True).fit(vec_train)\n",
    "    vec_columns = [f'component{i + 1}' for i in range(decomposer.n_components_)]\n",
    "    vec_train = pd.DataFrame(data=decomposer.transform(vec_train), columns=vec_columns)\n",
    "    vec_test = pd.DataFrame(data=decomposer.transform(vec_test), columns=vec_columns)\n",
    "elif DECOMPOSER == 'svd':\n",
    "    n_components = min(100, int(vec_train.shape[1] / 2))\n",
    "    decomposer = TruncatedSVD(n_components=n_components, random_state=SEED).fit(vec_train)\n",
    "    vec_columns = [f'component{i + 1}' for i in range(n_components)]\n",
    "    vec_train = pd.DataFrame(data=decomposer.transform(vec_train), columns=vec_columns)\n",
    "    vec_test = pd.DataFrame(data=decomposer.transform(vec_test), columns=vec_columns)\n",
    "else:\n",
    "    raise ValueError(DECOMPOSER)\n",
    "vec_train.shape, vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15296b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vec_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = pd.concat([X, vec_train], axis=1)\n",
    "X_test = pd.concat([X_test, vec_test], axis=1)\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8fac8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-caf2a2668095>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                      \u001b[1;33m+\u001b[0m \u001b[0mint_flag_features\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                      \u001b[1;33m+\u001b[0m \u001b[0marea_features\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                      \u001b[1;33m+\u001b[0m \u001b[0mvec_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vec_columns' is not defined"
     ]
    }
   ],
   "source": [
    "passthrough_features = amenity_onehot_features \\\n",
    "                     + amenity_components_columns \\\n",
    "                     + int_flag_features \\\n",
    "                     + area_features \\\n",
    "                     + vec_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough_features = amenity_onehot_features \\\n",
    "                     + amenity_components_columns \\\n",
    "                     + int_flag_features \\\n",
    "                     + area_features \\\n",
    "                     + vec_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad60d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(continuous_features, discrete_features, passthrough_features, random_state):\n",
    "    continuous_preprocessor = Pipeline(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "    categorical_preprocessor = OneHotEncoder(handle_unknown='ignore')\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('categorical', categorical_preprocessor, continuous_features),\n",
    "            ('continuous', continuous_preprocessor, discrete_features),\n",
    "            ('others', 'passthrough', passthrough_features)\n",
    "        ]\n",
    "    )\n",
    "    return Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(random_state=random_state,\n",
    "                                              max_iter=1000,\n",
    "                                              n_jobs=-1,\n",
    "                                              class_weight='balanced'))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660068a",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5066447",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5956148",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min, y_max = train['y'].min(), train['y'].max()  # clipping に必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットは対数変換する\n",
    "y = train.set_index('id').loc[X['id']]['y']\n",
    "y_log = np.log(y)\n",
    "y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cc623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-fold 用\n",
    "y_labels = pd.cut(y_log, bins=3, labels=range(3))\n",
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_threshold = np.log(Y_THRESHOLD)\n",
    "y_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3551cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = X_test['id'].values\n",
    "id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e9a18",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "for i, (train_idx, vaild_idx) in enumerate(splitter.split(X=X, y=y_labels)):\n",
    "    num_fold = i + 1\n",
    "    logger.debug('Start fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))\n",
    "\n",
    "    # 訓練データと検証データに分割\n",
    "    id_train = X.iloc[train_idx]['id'].values\n",
    "    X_train = X.iloc[train_idx].drop(columns=['id'])\n",
    "    y_train = y_log[train_idx].values\n",
    "    id_valid = X.iloc[vaild_idx]['id'].values\n",
    "    X_valid = X.iloc[vaild_idx].drop(columns=['id'])\n",
    "    y_valid = y_log[vaild_idx].values\n",
    "    \n",
    "    # モデルの訓練\n",
    "    ## 分類モデルの訓練\n",
    "    with timer('Training: classifier', logger):\n",
    "        y_train_clf = 1 * (y_train > y_threshold)\n",
    "        y_valid_clf = 1 * (y_valid > y_threshold)\n",
    "        classifier = get_classifier(continuous_features=continuous_features,\n",
    "                                    discrete_features=discrete_features,\n",
    "                                    passthrough_features=passthrough_features,\n",
    "                                    random_state=SEED)\n",
    "        classifier.fit(X_train, y_train_clf)\n",
    "    ## 分類モデルの評価\n",
    "    with timer('Evaluate classifier', logger):\n",
    "        pred_train_clf = classifier.predict(X_train)\n",
    "        logger.debug('Training f1 score: {:.6f}'.format(f1_score(y_train_clf, pred_train_clf)))\n",
    "        logger.debug('Training precision: {:.6f}'.format(precision_score(y_train_clf, pred_train_clf)))\n",
    "        pred_valid_clf = classifier.predict(X_valid)\n",
    "        logger.debug('Validation f1 score: {:.6f}'.format(f1_score(y_valid_clf, pred_valid_clf)))\n",
    "        logger.debug('Validation precision: {:.6f}'.format(precision_score(y_valid_clf, pred_valid_clf)))\n",
    "    ## `y` 予測モデルの訓練\n",
    "    with timer('Training regressor', logger):\n",
    "        class1_mask = y_train > y_threshold\n",
    "        estimator0 = LGBMRegressor(n_estimators=300,\n",
    "                                   random_state=SEED,\n",
    "                                   n_jobs=-1,\n",
    "                                   learning_rate=0.1,\n",
    "                                   importance_type='gain')\n",
    "        estimator0.fit(X_train[~class1_mask], y_train[~class1_mask], categorical_feature=discrete_features)\n",
    "        estimator1 = LGBMRegressor(n_estimators=300,\n",
    "                                   random_state=SEED,\n",
    "                                   n_jobs=-1,\n",
    "                                   learning_rate=0.1,\n",
    "                                   importance_type='gain')\n",
    "        estimator1.fit(X_train[class1_mask], y_train[class1_mask], categorical_feature=discrete_features)\n",
    "        \n",
    "    # 予測結果を保存する\n",
    "    with timer('Prediction', logger):\n",
    "        # 訓練データ\n",
    "        proba_train = classifier.predict_proba(X_train)\n",
    "        pred_train0 = estimator0.predict(X_train)\n",
    "        pred_train1 = estimator1.predict(X_train)\n",
    "        pred_train = pred_train0 * proba_train[:, 0] + pred_train1 * proba_train[:, 1]\n",
    "        pred_train = pd.DataFrame(data=pred_train, columns=['pred'])\n",
    "        pred_train['pred'] = np.exp(pred_train['pred'])\n",
    "        pred_train['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "        # 検証データ\n",
    "        proba_valid = classifier.predict_proba(X_valid)\n",
    "        pred_valid0 = estimator0.predict(X_valid)\n",
    "        pred_valid1 = estimator1.predict(X_valid)\n",
    "        pred_valid = pred_valid0 * proba_valid[:, 0] + pred_valid1 * proba_valid[:, 1]\n",
    "        pred_valid = pd.DataFrame(data=pred_valid, columns=['pred'])\n",
    "        pred_valid['pred'] = np.exp(pred_valid['pred'])\n",
    "        pred_valid['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "        # テストデータ\n",
    "        proba_test = classifier.predict_proba(X_test.drop(columns=['id']))\n",
    "        pred_test0 = estimator0.predict(X_test.drop(columns=['id']))\n",
    "        pred_test1 = estimator1.predict(X_test.drop(columns=['id']))\n",
    "        pred_test = pred_test0 * proba_test[:, 0] + pred_test1 * proba_test[:, 1]\n",
    "        pred_test = pd.DataFrame(data=pred_test, columns=['pred'])\n",
    "        pred_test['pred'] = np.exp(pred_test['pred'])\n",
    "        pred_test['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "    with timer('Save prediction', logger):\n",
    "        ## 訓練データ\n",
    "        pred_train['id'] = id_train\n",
    "        pred_train.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_training.csv'), index=False)\n",
    "        ## 検証データ\n",
    "        pred_valid['id'] = id_valid\n",
    "        pred_valid.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv'), index=False)\n",
    "        ## テストデータ\n",
    "        pred_test['id'] = id_test\n",
    "        pred_test.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_test.csv'), index=False)\n",
    "    ## モデルの保存\n",
    "    with timer('Save model', logger):\n",
    "        filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.pkl')\n",
    "        with open(filepath_fold_model, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'class0': estimator0,\n",
    "                'class1': estimator1,\n",
    "                'classifier': classifier\n",
    "            }, f)\n",
    "    logger.debug('Complete fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae9866",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ee9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = partial(mse, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86219793",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    pred_df = pd.read_csv(os.path.join(output_dir, f'cv_fold{num_fold}_training.csv'))\n",
    "    pred_df['actual'] = train.loc[pred_df['id'], 'y'].values\n",
    "    cv_loss = rmse(pred_df['actual'], pred_df['pred'])\n",
    "    logger.info('CV fold {} training loss={:.7f}'.format(num_fold, cv_loss))\n",
    "    metrics['train_losses'].append(cv_loss)\n",
    "    pred_train_dfs.append(pred_df)\n",
    "\n",
    "metrics['train_losses_avg'] = np.mean(metrics['train_losses'])\n",
    "metrics['train_losses_std'] = np.std(metrics['train_losses'])\n",
    "\n",
    "logger.info('CV training loss: average={:.7f}, std={:.7f}' \\\n",
    "            .format(metrics['train_losses_avg'], metrics['train_losses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36960aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pd.concat(pred_train_dfs).groupby('id').sum()\n",
    "pred_train = pred_train / N_SPLITS\n",
    "pred_train['actual'] = train.loc[pred_train.index, 'y'].values\n",
    "pred_train.to_csv(os.path.join(output_dir, 'prediction_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c512ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = rmse(pred_train['actual'], pred_train['pred'])\n",
    "metrics['train_loss'] = train_loss\n",
    "logger.info('Training loss: {:.7f}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc438710",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73167f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    pred_df = pd.read_csv(os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv'))\n",
    "    pred_df['actual'] = train.loc[pred_df['id'], 'y'].values\n",
    "    cv_loss = rmse(pred_df['actual'], pred_df['pred'])\n",
    "    logger.info('CV fold {} validation loss={:.7f}'.format(num_fold, cv_loss))\n",
    "    metrics['valid_losses'].append(cv_loss)\n",
    "    pred_valid_dfs.append(pred_df)\n",
    "\n",
    "metrics['valid_losses_avg'] = np.mean(metrics['valid_losses'])\n",
    "metrics['valid_losses_std'] = np.std(metrics['valid_losses'])\n",
    "\n",
    "logger.info('CV validation loss: average={:.7f}, std={:.7f}' \\\n",
    "            .format(metrics['valid_losses_avg'], metrics['valid_losses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pd.concat(pred_valid_dfs).groupby('id').sum()\n",
    "pred_valid = pred_valid / N_SPLITS\n",
    "pred_valid['actual'] = train.loc[pred_valid.index, 'y'].values\n",
    "pred_valid.to_csv(os.path.join(output_dir, 'prediction_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b303ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = rmse(pred_valid['actual'], pred_valid['pred'])\n",
    "metrics['valid_loss'] = valid_loss\n",
    "logger.info('Validation loss: {:.7f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edaccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab51ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12.5, 6.5))\n",
    "plt.suptitle('Actual vs Prediction')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.scatterplot(pred_train['actual'], pred_train['pred'])\n",
    "plt.plot(pred_train['actual'], pred_train['actual'], color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "ax.set_title('Training set');\n",
    "# plt.axes().set_aspect('equal')\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.scatterplot(pred_valid['actual'], pred_valid['pred'])\n",
    "plt.plot(pred_valid['actual'], pred_valid['actual'], color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "plt.title('Validation set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['train_losses'], label='training set')\n",
    "plt.plot(metrics['valid_losses'], label='validation set')\n",
    "plt.title('Training/Validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0723976",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_dfs = [pd.read_csv(os.path.join(output_dir, f'cv_fold{i + 1}_test.csv')) for i in range(N_SPLITS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f587433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pd.concat(pred_test_dfs).groupby('id').sum()\n",
    "pred_test = pred_test / N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.to_csv(os.path.join(output_dir, f'{EXPERIMENT}_submission.csv'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fe0ce",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6996763",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_min, ylim_max = np.log(y_min * CLIP_LOWER_RATE), np.log(y_max * CLIP_UPPER_RATE)\n",
    "fig = plt.figure(figsize=(10.5, 10.5))\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = sns.histplot(y_log)\n",
    "ax.set_title('Actual y, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "ax = sns.histplot(np.log(pred_train['pred']))\n",
    "ax.set_title('Training set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "ax = sns.histplot(np.log(pred_valid['pred']))\n",
    "ax.set_title('Validation set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "ax = sns.histplot(np.log(pred_test['pred']))\n",
    "ax.set_title('Test set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "fig.savefig(os.path.join(output_dir, 'figure.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5404d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Complete({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7159b2",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c026d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0904998",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i, (train_idx, vaild_idx) in enumerate(splitter.split(X=X, y=y_labels)):\n",
    "    num_fold = i + 1\n",
    "\n",
    "    ## モデルの保存\n",
    "    filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.pkl')\n",
    "    with open(filepath_fold_model, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 21.5))\n",
    "for i in range(N_SPLITS):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    imp_df = pd.DataFrame(data=model['class0'].feature_importances_, columns=['importance'])\n",
    "    imp_df['feature'] = X_train.columns.tolist()\n",
    "    imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "    sns.barplot(data=imp_df.head(25), y='feature', x='importance')\n",
    "fig.savefig(os.path.join(output_dir, 'feature_importance0.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 21.5))\n",
    "for i in range(N_SPLITS):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    imp_df = pd.DataFrame(data=model['class1'].feature_importances_, columns=['importance'])\n",
    "    imp_df['feature'] = X_train.columns.tolist()\n",
    "    imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "    sns.barplot(data=imp_df.head(25), y='feature', x='importance')\n",
    "fig.savefig(os.path.join(output_dir, 'feature_importance0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e85f75",
   "metadata": {},
   "source": [
    "# Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pd.read_csv(os.path.join(output_dir, 'prediction_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23845459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid['diff'] = pred_valid['pred'] - pred_valid['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.axes().set_aspect('equal')\n",
    "ax = sns.scatterplot(data=pred_valid, x='actual', y='pred')\n",
    "ax = sns.lineplot(data=pred_valid, x='actual', y='actual', color='red')\n",
    "fig.savefig(os.path.join(output_dir, 'compare_actual_prediction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid.describe()\n",
    "sns.histplot(data=pred_valid, x='diff')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(X, pred_valid[['id', 'diff']]).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sweetviz\n",
    "report = sweetviz.analyze(diff_df, target_feat='diff', pairwise_analysis='off')\n",
    "report.show_html(os.path.join(output_dir, ('sweetviz_error_report.html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_columns = diff_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sort_values('diff').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14761313",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sort_values('diff').tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb053a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.corr().sort_values('diff')['diff'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "estimator = LGBMRegressor(random_state=SEED, n_jobs=-1, importance_type='gain').fit(diff_df.drop(columns=['diff']), diff_df['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345caab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame(data=estimator.feature_importances_, columns=['importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df['feature'] = estimator.feature_name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f02c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.to_csv(os.path.join(output_dir, 'diff_feature_importances.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a26a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
