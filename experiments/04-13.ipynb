{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81000a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = '04-13'\n",
    "N_SPLITS = 5\n",
    "SEED = 1\n",
    "CLIP_UPPER_RATE = 1.5\n",
    "CLIP_LOWER_RATE = 1.0\n",
    "Y_THRESHOLD = 200\n",
    "EMBEDDER = 'tfidf'\n",
    "DECOMPOSER = 'pca'  # 'pca', 'svd' or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import configparser\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "SINCE = time.time()\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error as mse, f1_score, precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "INI_FILEPATH = os.path.join(os.path.expanduser('~'), 'aiquest2021-assesment', 'config.ini')\n",
    "config.read(INI_FILEPATH)\n",
    "if config['FOLDER']['SCRIPTS'] not in sys.path:\n",
    "    sys.path.append(config['FOLDER']['SCRIPTS'])\n",
    "from logging_util import get_logger, timer\n",
    "from feature_engineering import make_or_load_features, make_or_load_description_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(config['FOLDER']['EXPERIMENTS'], EXPERIMENT)\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, os.path.join(output_dir, 'log.log'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7074a",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f33420",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('Load dataset', logger):\n",
    "    train = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'test.csv'))\n",
    "    sample_submit = pd.read_csv(os.path.join(config['FOLDER']['INPUT'], 'sample_submit.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7aa03",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(config['FOLDER']['FEATURES'], 'train_features.csv')\n",
    "test_path = os.path.join(config['FOLDER']['FEATURES'], 'test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = make_or_load_features(train, test, train_path, test_path, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns = [re.sub(r\"[:;/']\", '', c) for c in X.columns]\n",
    "X_test.columns = [re.sub(r\"[:;/']\", '', c) for c in X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ffc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "area_features = ['latitude', 'longitude']\n",
    "\n",
    "# Onehot encoding\n",
    "categorical_features = ['cancellation_policy', 'bed_type', 'city', 'neighbourhood', 'property_type',\n",
    "                        'room_type', 'zipcode5', 'zipcode_1st_digit']\n",
    "\n",
    "# They do not need to be encoded\n",
    "int_flag_features = ['cleaning_fee', 'host_has_profile_pic', 'host_identity_verified',\n",
    "                     'instant_bookable', 'has_thumbnail', 'zipcode_imputed']\n",
    "\n",
    "# Already one-hot style\n",
    "amenity_onehot_features = [c for c in X.columns if c.startswith('has_') and c.endswith('_amenity')]\n",
    "\n",
    "discrete_features = categorical_features + int_flag_features + amenity_onehot_features\n",
    "\n",
    "# Scaling, transformation\n",
    "continuous_features = [\n",
    "    c for c in X.columns\n",
    "    if c not in discrete_features + area_features + ['id', 'y']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reduce dimension & whitening\n",
    "## training\n",
    "decomposer = PCA(n_components=0.8, random_state=SEED, whiten=True).fit(X[amenity_onehot_features])\n",
    "X_amenity_components = decomposer.transform(X[amenity_onehot_features])\n",
    "amenity_components_columns = [f'amenity_x{i + 1}' for i in range(decomposer.n_components_)]\n",
    "X_amenity_components = pd.DataFrame(data=X_amenity_components,\n",
    "                                    columns=amenity_components_columns)\n",
    "X = pd.concat([X, X_amenity_components], axis=1)\n",
    "# X.drop(columns=amenity_onehot_features, inplace=True)\n",
    "X_test_amenity_components = decomposer.transform(X_test[amenity_onehot_features])\n",
    "X_test_amenity_components = pd.DataFrame(data=X_test_amenity_components,\n",
    "                                         columns=amenity_components_columns)\n",
    "X_test = pd.concat([X_test, X_test_amenity_components], axis=1)\n",
    "# X_test.drop(columns=amenity_onehot_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vec_train, vec_test = make_or_load_description_vector(train=train,\n",
    "                                                      test=test,\n",
    "                                                      feature_dir=config['FOLDER']['FEATURES'],\n",
    "                                                      logger=logger,\n",
    "                                                      embedder=EMBEDDER)\n",
    "vec_train.shape, vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if DECOMPOSER is None:\n",
    "    assert(EMBEDDER not in ('count', 'tfidf'))\n",
    "    vec_columns = vec_train.columns.tolist()\n",
    "elif DECOMPOSER == 'pca':\n",
    "    decomposer = PCA(n_components=0.9, random_state=SEED, whiten=True).fit(vec_train)\n",
    "    vec_columns = [f'component{i + 1}' for i in range(decomposer.n_components_)]\n",
    "    vec_train = pd.DataFrame(data=decomposer.transform(vec_train), columns=vec_columns)\n",
    "    vec_test = pd.DataFrame(data=decomposer.transform(vec_test), columns=vec_columns)\n",
    "elif DECOMPOSER == 'svd':\n",
    "    n_components = min(100, int(vec_train.shape[1] / 2))\n",
    "    decomposer = TruncatedSVD(n_components=n_components, random_state=SEED).fit(vec_train)\n",
    "    vec_columns = [f'component{i + 1}' for i in range(n_components)]\n",
    "    vec_train = pd.DataFrame(data=decomposer.transform(vec_train), columns=vec_columns)\n",
    "    vec_test = pd.DataFrame(data=decomposer.transform(vec_test), columns=vec_columns)\n",
    "else:\n",
    "    raise ValueError(DECOMPOSER)\n",
    "vec_train.shape, vec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15296b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = pd.concat([X, vec_train], axis=1)\n",
    "X_test = pd.concat([X_test, vec_test], axis=1)\n",
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough_features = amenity_onehot_features \\\n",
    "                     + amenity_components_columns \\\n",
    "                     + int_flag_features \\\n",
    "                     + area_features \\\n",
    "                     + vec_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad60d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(continuous_features, discrete_features, passthrough_features, random_state):\n",
    "    continuous_preprocessor = Pipeline(\n",
    "        steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "    categorical_preprocessor = OneHotEncoder(handle_unknown='ignore')\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('categorical', categorical_preprocessor, continuous_features),\n",
    "            ('continuous', continuous_preprocessor, discrete_features),\n",
    "            ('others', 'passthrough', passthrough_features)\n",
    "        ]\n",
    "    )\n",
    "    return Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(random_state=random_state,\n",
    "                                              max_iter=1000,\n",
    "                                              n_jobs=-1,\n",
    "                                              class_weight='balanced'))\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660068a",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5066447",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5956148",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min, y_max = train['y'].min(), train['y'].max()  # clipping に必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットは対数変換する\n",
    "y = train.set_index('id').loc[X['id']]['y']\n",
    "y_log = np.log(y)\n",
    "y_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cc623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-fold 用\n",
    "y_labels = pd.cut(y_log, bins=3, labels=range(3))\n",
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_threshold = np.log(Y_THRESHOLD)\n",
    "y_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3551cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = X_test['id'].values\n",
    "id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e9a18",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "for i, (train_idx, vaild_idx) in enumerate(splitter.split(X=X, y=y_labels)):\n",
    "    num_fold = i + 1\n",
    "    logger.debug('Start fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))\n",
    "\n",
    "    # 訓練データと検証データに分割\n",
    "    id_train = X.iloc[train_idx]['id'].values\n",
    "    X_train = X.iloc[train_idx].drop(columns=['id'])\n",
    "    y_train = y_log[train_idx].values\n",
    "    id_valid = X.iloc[vaild_idx]['id'].values\n",
    "    X_valid = X.iloc[vaild_idx].drop(columns=['id'])\n",
    "    y_valid = y_log[vaild_idx].values\n",
    "    \n",
    "    # モデルの訓練\n",
    "    ## 分類モデルの訓練\n",
    "    with timer('Training: classifier', logger):\n",
    "        y_train_clf = 1 * (y_train > y_threshold)\n",
    "        y_valid_clf = 1 * (y_valid > y_threshold)\n",
    "        classifier = get_classifier(continuous_features=continuous_features,\n",
    "                                    discrete_features=discrete_features,\n",
    "                                    passthrough_features=passthrough_features,\n",
    "                                    random_state=SEED)\n",
    "        classifier.fit(X_train, y_train_clf)\n",
    "    ## 分類モデルの評価\n",
    "    with timer('Evaluate classifier', logger):\n",
    "        pred_train_clf = classifier.predict(X_train)\n",
    "        logger.debug('Training f1 score: {:.6f}'.format(f1_score(y_train_clf, pred_train_clf)))\n",
    "        logger.debug('Training precision: {:.6f}'.format(precision_score(y_train_clf, pred_train_clf)))\n",
    "        pred_valid_clf = classifier.predict(X_valid)\n",
    "        logger.debug('Validation f1 score: {:.6f}'.format(f1_score(y_valid_clf, pred_valid_clf)))\n",
    "        logger.debug('Validation precision: {:.6f}'.format(precision_score(y_valid_clf, pred_valid_clf)))\n",
    "    ## `y` 予測モデルの訓練\n",
    "    with timer('Training regressor', logger):\n",
    "        class1_mask = y_train > y_threshold\n",
    "        estimator0 = LGBMRegressor(n_estimators=300,\n",
    "                                   random_state=SEED,\n",
    "                                   n_jobs=-1,\n",
    "                                   learning_rate=0.1,\n",
    "                                   importance_type='gain')\n",
    "        estimator0.fit(X_train[~class1_mask], y_train[~class1_mask], categorical_feature=discrete_features)\n",
    "        estimator1 = LGBMRegressor(n_estimators=300,\n",
    "                                   random_state=SEED,\n",
    "                                   n_jobs=-1,\n",
    "                                   learning_rate=0.1,\n",
    "                                   importance_type='gain')\n",
    "        estimator1.fit(X_train[class1_mask], y_train[class1_mask], categorical_feature=discrete_features)\n",
    "        \n",
    "    # 予測結果を保存する\n",
    "    with timer('Prediction', logger):\n",
    "        # 訓練データ\n",
    "        proba_train = classifier.predict_proba(X_train)\n",
    "        pred_train0 = estimator0.predict(X_train)\n",
    "        pred_train1 = estimator1.predict(X_train)\n",
    "        pred_train = pred_train0 * proba_train[:, 0] + pred_train1 * proba_train[:, 1]\n",
    "        pred_train = pd.DataFrame(data=pred_train, columns=['pred'])\n",
    "        pred_train['pred'] = np.exp(pred_train['pred'])\n",
    "        pred_train['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "        # 検証データ\n",
    "        proba_valid = classifier.predict_proba(X_valid)\n",
    "        pred_valid0 = estimator0.predict(X_valid)\n",
    "        pred_valid1 = estimator1.predict(X_valid)\n",
    "        pred_valid = pred_valid0 * proba_valid[:, 0] + pred_valid1 * proba_valid[:, 1]\n",
    "        pred_valid = pd.DataFrame(data=pred_valid, columns=['pred'])\n",
    "        pred_valid['pred'] = np.exp(pred_valid['pred'])\n",
    "        pred_valid['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "        # テストデータ\n",
    "        proba_test = classifier.predict_proba(X_test.drop(columns=['id']))\n",
    "        pred_test0 = estimator0.predict(X_test.drop(columns=['id']))\n",
    "        pred_test1 = estimator1.predict(X_test.drop(columns=['id']))\n",
    "        pred_test = pred_test0 * proba_test[:, 0] + pred_test1 * proba_test[:, 1]\n",
    "        pred_test = pd.DataFrame(data=pred_test, columns=['pred'])\n",
    "        pred_test['pred'] = np.exp(pred_test['pred'])\n",
    "        pred_test['pred'].clip(lower=y_min * CLIP_LOWER_RATE, upper=y_max * CLIP_UPPER_RATE, inplace=True)\n",
    "    with timer('Save prediction', logger):\n",
    "        ## 訓練データ\n",
    "        pred_train['id'] = id_train\n",
    "        pred_train.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_training.csv'), index=False)\n",
    "        ## 検証データ\n",
    "        pred_valid['id'] = id_valid\n",
    "        pred_valid.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv'), index=False)\n",
    "        ## テストデータ\n",
    "        pred_test['id'] = id_test\n",
    "        pred_test.to_csv(os.path.join(output_dir, f'cv_fold{num_fold}_test.csv'), index=False)\n",
    "    ## モデルの保存\n",
    "    with timer('Save model', logger):\n",
    "        filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.pkl')\n",
    "        with open(filepath_fold_model, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'class0': estimator0,\n",
    "                'class1': estimator1,\n",
    "                'classifier': classifier\n",
    "            }, f)\n",
    "    logger.debug('Complete fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae9866",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ee9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = partial(mse, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86219793",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    pred_df = pd.read_csv(os.path.join(output_dir, f'cv_fold{num_fold}_training.csv'))\n",
    "    pred_df['actual'] = train.loc[pred_df['id'], 'y'].values\n",
    "    cv_loss = rmse(pred_df['actual'], pred_df['pred'])\n",
    "    logger.info('CV fold {} training loss={:.7f}'.format(num_fold, cv_loss))\n",
    "    metrics['train_losses'].append(cv_loss)\n",
    "    pred_train_dfs.append(pred_df)\n",
    "\n",
    "metrics['train_losses_avg'] = np.mean(metrics['train_losses'])\n",
    "metrics['train_losses_std'] = np.std(metrics['train_losses'])\n",
    "\n",
    "logger.info('CV training loss: average={:.7f}, std={:.7f}' \\\n",
    "            .format(metrics['train_losses_avg'], metrics['train_losses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36960aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pd.concat(pred_train_dfs).groupby('id').sum()\n",
    "pred_train = pred_train / N_SPLITS\n",
    "pred_train['actual'] = train.loc[pred_train.index, 'y'].values\n",
    "pred_train.to_csv(os.path.join(output_dir, 'prediction_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c512ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = rmse(pred_train['actual'], pred_train['pred'])\n",
    "metrics['train_loss'] = train_loss\n",
    "logger.info('Training loss: {:.7f}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc438710",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73167f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    pred_df = pd.read_csv(os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv'))\n",
    "    pred_df['actual'] = train.loc[pred_df['id'], 'y'].values\n",
    "    cv_loss = rmse(pred_df['actual'], pred_df['pred'])\n",
    "    logger.info('CV fold {} validation loss={:.7f}'.format(num_fold, cv_loss))\n",
    "    metrics['valid_losses'].append(cv_loss)\n",
    "    pred_valid_dfs.append(pred_df)\n",
    "\n",
    "metrics['valid_losses_avg'] = np.mean(metrics['valid_losses'])\n",
    "metrics['valid_losses_std'] = np.std(metrics['valid_losses'])\n",
    "\n",
    "logger.info('CV validation loss: average={:.7f}, std={:.7f}' \\\n",
    "            .format(metrics['valid_losses_avg'], metrics['valid_losses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pd.concat(pred_valid_dfs).groupby('id').sum()\n",
    "pred_valid = pred_valid / N_SPLITS\n",
    "pred_valid['actual'] = train.loc[pred_valid.index, 'y'].values\n",
    "pred_valid.to_csv(os.path.join(output_dir, 'prediction_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b303ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = rmse(pred_valid['actual'], pred_valid['pred'])\n",
    "metrics['valid_loss'] = valid_loss\n",
    "logger.info('Validation loss: {:.7f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edaccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab51ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12.5, 6.5))\n",
    "plt.suptitle('Actual vs Prediction')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.scatterplot(pred_train['actual'], pred_train['pred'])\n",
    "plt.plot(pred_train['actual'], pred_train['actual'], color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "ax.set_title('Training set');\n",
    "# plt.axes().set_aspect('equal')\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.scatterplot(pred_valid['actual'], pred_valid['pred'])\n",
    "plt.plot(pred_valid['actual'], pred_valid['actual'], color='black', linewidth=0.5)\n",
    "ax.set_xlim(0, 2000)\n",
    "ax.set_ylim(0, 2000)\n",
    "plt.title('Validation set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics['train_losses'], label='training set')\n",
    "plt.plot(metrics['valid_losses'], label='validation set')\n",
    "plt.title('Training/Validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0723976",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_dfs = [pd.read_csv(os.path.join(output_dir, f'cv_fold{i + 1}_test.csv')) for i in range(N_SPLITS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f587433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pd.concat(pred_test_dfs).groupby('id').sum()\n",
    "pred_test = pred_test / N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.to_csv(os.path.join(output_dir, f'{EXPERIMENT}_submission.csv'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fe0ce",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6996763",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim_min, ylim_max = np.log(y_min * CLIP_LOWER_RATE), np.log(y_max * CLIP_UPPER_RATE)\n",
    "fig = plt.figure(figsize=(10.5, 10.5))\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = sns.histplot(y_log)\n",
    "ax.set_title('Actual y, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "ax = sns.histplot(np.log(pred_train['pred']))\n",
    "ax.set_title('Training set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "ax = sns.histplot(np.log(pred_valid['pred']))\n",
    "ax.set_title('Validation set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "ax = sns.histplot(np.log(pred_test['pred']))\n",
    "ax.set_title('Test set, log-scaled')\n",
    "ax.set_xlim(ylim_min, ylim_max)\n",
    "sns.despine()\n",
    "\n",
    "fig.savefig(os.path.join(output_dir, 'figure.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5404d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Complete({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7159b2",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c026d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0904998",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i, (train_idx, vaild_idx) in enumerate(splitter.split(X=X, y=y_labels)):\n",
    "    num_fold = i + 1\n",
    "\n",
    "    ## モデルの保存\n",
    "    filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.pkl')\n",
    "    with open(filepath_fold_model, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 21.5))\n",
    "for i in range(N_SPLITS):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    imp_df = pd.DataFrame(data=model['class0'].feature_importances_, columns=['importance'])\n",
    "    imp_df['feature'] = X_train.columns.tolist()\n",
    "    imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "    sns.barplot(data=imp_df.head(25), y='feature', x='importance')\n",
    "fig.savefig(os.path.join(output_dir, 'feature_importance0.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 21.5))\n",
    "for i in range(N_SPLITS):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    imp_df = pd.DataFrame(data=model['class1'].feature_importances_, columns=['importance'])\n",
    "    imp_df['feature'] = X_train.columns.tolist()\n",
    "    imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "    sns.barplot(data=imp_df.head(25), y='feature', x='importance')\n",
    "fig.savefig(os.path.join(output_dir, 'feature_importance0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e85f75",
   "metadata": {},
   "source": [
    "# Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = pd.read_csv(os.path.join(output_dir, 'prediction_valid.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23845459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid['diff'] = pred_valid['pred'] - pred_valid['actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.axes().set_aspect('equal')\n",
    "ax = sns.scatterplot(data=pred_valid, x='actual', y='pred')\n",
    "ax = sns.lineplot(data=pred_valid, x='actual', y='actual', color='red')\n",
    "fig.savefig(os.path.join(output_dir, 'compare_actual_prediction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid.describe()\n",
    "sns.histplot(data=pred_valid, x='diff')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(X, pred_valid[['id', 'diff']]).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sweetviz\n",
    "report = sweetviz.analyze(diff_df, target_feat='diff', pairwise_analysis='off')\n",
    "report.show_html(os.path.join(output_dir, ('sweetviz_error_report.html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_columns = diff_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sort_values('diff').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14761313",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.sort_values('diff').tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb053a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.corr().sort_values('diff')['diff'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "estimator = LGBMRegressor(random_state=SEED, n_jobs=-1, importance_type='gain').fit(diff_df.drop(columns=['diff']), diff_df['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345caab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame(data=estimator.feature_importances_, columns=['importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bbc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df['feature'] = estimator.feature_name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.sort_values(['importance', 'feature'], ascending=False, inplace=True)\n",
    "imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f02c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df.to_csv(os.path.join(output_dir, 'diff_feature_importances.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a26a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
